{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from common.utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = Utils.get_tongyi_key()\n",
    "#llm = Tongyi(model_name= 'qwen-max-1201', temperature = 0)\n",
    "from common.CustomLLM import QwenLLM\n",
    "url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "token = Utils.get_tongyi_key()\n",
    "\n",
    "qwen = QwenLLM(url= url, model='qwen-max-1201', token=token, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"/Users/collin/.cache/modelscope/hub/AI-ModelScope/bge-base-zh-v1.5\"\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings' : True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    encode_kwargs = encode_kwargs,\n",
    "    query_instruction = \"为这个句子生成表示用于检索相关文章：\",#\"Represent this sentence for searching relevant passages:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12381625175476074\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "embedding_model.embed_query(\"\"\"Dear Alumni,\n",
    "\n",
    "We would like to invite you to take part in the University’s website user feedback questionnaire. Your thoughts and opinions are highly valued as crucial feedback for ongoing efforts to enhance the website. We appreciate your time and assure you that all responses will be kept strictly confidential.\n",
    "\n",
    "The questionnaire should take approximately 3 minutes to complete and can be accessed through the link below. As a token of our gratitude, the first 100 participants will receive a silicone travel tag as a reward. An email will be sent to winners and details of the redemption will be provided on 30 Jan 2024. Please kindly complete the questionnaire before 12 Jan 2024.\"\"\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine.sub_question_query_engine import SubQuestionQueryEngineCustom\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm = qwen, embed_model = embedding_model, chunk_size=2048, chunk_overlap= 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collin/miniconda3/envs/llm/lib/python3.9/site-packages/pypdf/_cmap.py:183: PdfReadWarning: Advanced encoding /ETenms-B5-H not implemented yet\n",
      "  warnings.warn(\n",
      "/Users/collin/miniconda3/envs/llm/lib/python3.9/site-packages/pypdf/_cmap.py:183: PdfReadWarning: Advanced encoding /UniCNS-UTF16-H not implemented yet\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dbs_report = SimpleDirectoryReader(input_files = ['./data/230327-annual-report-and-accounts-2022-cn.pdf']).load_data()#input_dir=\"./data/paul_graham/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index and query engine\n",
    "vector_dbs_idnex = VectorStoreIndex.from_documents(\n",
    "    dbs_report, service_context=service_context\n",
    ")\n",
    "\n",
    "vector_dbs_idnex.storage_context.persist(persist_dir = './data/subquery/hsbc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_dbs_doc = StorageContext.from_defaults(persist_dir = './data/subquery/hsbc')\n",
    "storage_dbs_index = load_index_from_storage(storage_dbs_doc, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=storage_dbs_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"滙豐控股有限公司2022年報及賬目\",\n",
    "            description=\"提供滙豐控股有限公司2022年財報的信息\"\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngineCustom.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    verbose=True,\n",
    "    use_async=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query('策略表現指標')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 sub questions.\n",
      "Generated 1 Sub Questions.\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;237;90;200m[滙豐控股有限公司2022年報及賬目] Q: 請問貴公司今年的策略表現為何？\n",
      "\u001b[0m[滙豐控股有限公司2022年報及賬目] Q: 請問貴公司今年的策略表現為何？\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;237;90;200m[滙豐控股有限公司2022年報及賬目] A:  對不起，我找不到答案\n",
      "\u001b[0m[滙豐控股有限公司2022年報及賬目] A:  對不起，我找不到答案\n",
      "\n",
      "-----\n",
      " 根據提供的上下文，無法確定「策略表現」指標的具體定義或衡量標準。若要獲得更詳細的答案，建議向該公司詢問其評估策略表現的方式和相關數據。\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print('-' * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index==0.9.15.post2 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abd'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'abd'\n",
    "s.replace('a', '')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Optional, Sequence, cast\n",
    "\n",
    "from llama_index.async_utils import run_async_tasks\n",
    "from llama_index.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.callbacks.base import CallbackManager\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.core import BaseQueryEngine\n",
    "from llama_index.prompts.mixin import PromptMixinType\n",
    "from llama_index.question_gen.llm_generators import LLMQuestionGenerator\n",
    "from llama_index.question_gen.openai_generator import OpenAIQuestionGenerator\n",
    "from llama_index.question_gen.types import BaseQuestionGenerator, SubQuestion\n",
    "from llama_index.response.schema import RESPONSE_TYPE\n",
    "from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer\n",
    "from llama_index.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.service_context import ServiceContext\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "from llama_index.utils import get_color_mapping, print_text\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SubQuestionAnswerPair(BaseModel):\n",
    "    \"\"\"\n",
    "    Pair of the sub question and optionally its answer (if its been answered yet).\n",
    "    \"\"\"\n",
    "\n",
    "    sub_q: SubQuestion\n",
    "    answer: Optional[str] = None\n",
    "    sources: List[NodeWithScore] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class SubQuestionQueryEngineCustom(BaseQueryEngine):\n",
    "    \"\"\"Sub question query engine.\n",
    "\n",
    "    A query engine that breaks down a complex query (e.g. compare and contrast) into\n",
    "        many sub questions and their target query engine for execution.\n",
    "        After executing all sub questions, all responses are gathered and sent to\n",
    "        response synthesizer to produce the final response.\n",
    "\n",
    "    Args:\n",
    "        question_gen (BaseQuestionGenerator): A module for generating sub questions\n",
    "            given a complex question and tools.\n",
    "        response_synthesizer (BaseSynthesizer): A response synthesizer for\n",
    "            generating the final response\n",
    "        query_engine_tools (Sequence[QueryEngineTool]): Tools to answer the\n",
    "            sub questions.\n",
    "        verbose (bool): whether to print intermediate questions and answers.\n",
    "            Defaults to True\n",
    "        use_async (bool): whether to execute the sub questions with asyncio.\n",
    "            Defaults to True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        question_gen: BaseQuestionGenerator,\n",
    "        response_synthesizer: BaseSynthesizer,\n",
    "        query_engine_tools: Sequence[QueryEngineTool],\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        verbose: bool = True,\n",
    "        use_async: bool = False,\n",
    "    ) -> None:\n",
    "        self._question_gen = question_gen\n",
    "        self._response_synthesizer = response_synthesizer\n",
    "        self._metadatas = [x.metadata for x in query_engine_tools]\n",
    "        self._query_engines = {\n",
    "            tool.metadata.name: tool.query_engine for tool in query_engine_tools\n",
    "        }\n",
    "        self._verbose = verbose\n",
    "        self._use_async = use_async\n",
    "        super().__init__(callback_manager)\n",
    "\n",
    "    def _get_prompt_modules(self) -> PromptMixinType:\n",
    "        \"\"\"Get prompt sub-modules.\"\"\"\n",
    "        return {\n",
    "            \"question_gen\": self._question_gen,\n",
    "            \"response_synthesizer\": self._response_synthesizer,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        query_engine_tools: Sequence[QueryEngineTool],\n",
    "        question_gen: Optional[BaseQuestionGenerator] = None,\n",
    "        response_synthesizer: Optional[BaseSynthesizer] = None,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "        verbose: bool = True,\n",
    "        use_async: bool = True,\n",
    "    ) -> \"SubQuestionQueryEngine\":\n",
    "        callback_manager = None\n",
    "        if service_context is not None:\n",
    "            callback_manager = service_context.callback_manager\n",
    "        elif len(query_engine_tools) > 0:\n",
    "            callback_manager = query_engine_tools[0].query_engine.callback_manager\n",
    "\n",
    "        if question_gen is None:\n",
    "            if service_context is None:\n",
    "                # use default openai model that supports function calling API\n",
    "                question_gen = OpenAIQuestionGenerator.from_defaults()\n",
    "            else:\n",
    "                # try to use OpenAI function calling based question generator.\n",
    "                # if incompatible, use general LLM question generator\n",
    "                try:\n",
    "                    question_gen = OpenAIQuestionGenerator.from_defaults(\n",
    "                        llm=service_context.llm\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    question_gen = LLMQuestionGenerator.from_defaults(\n",
    "                        service_context=service_context\n",
    "                    )\n",
    "\n",
    "        synth = response_synthesizer or get_response_synthesizer(\n",
    "            callback_manager=callback_manager,\n",
    "            service_context=service_context,\n",
    "            use_async=use_async,\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            question_gen,\n",
    "            synth,\n",
    "            query_engine_tools,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=verbose,\n",
    "            use_async=use_async,\n",
    "        )\n",
    "\n",
    "    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            sub_questions = self._question_gen.generate(self._metadatas, query_bundle)\n",
    "\n",
    "            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n",
    "\n",
    "            if self._verbose:\n",
    "                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n",
    "            yield f\"Generated {len(sub_questions)} Sub Questions.\\n\"\n",
    "\n",
    "            if self._use_async:\n",
    "                tasks = [\n",
    "                    self._aquery_subq(sub_q, color=colors[str(ind)])\n",
    "                    for ind, sub_q in enumerate(sub_questions)\n",
    "                ]\n",
    "\n",
    "                qa_pairs_all = run_async_tasks(tasks)\n",
    "                qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n",
    "            else:\n",
    "                qa_pairs_all = []\n",
    "                #    self._query_subq(sub_q, color=colors[str(ind)])\n",
    "                #    for ind, sub_q in enumerate(sub_questions)\n",
    "                #]\n",
    "                #print(\"Question\", sub_questions)\n",
    "                for ind, sub_q in enumerate(sub_questions):\n",
    "                    for item in self._query_subq(sub_q, color=colors[str(ind)]):\n",
    "                        #print(ind, item)\n",
    "                        if isinstance(item, str):\n",
    "                            yield item\n",
    "                        else:\n",
    "                            #print('itemss')\n",
    "                            qa_pairs_all.append(item)\n",
    "                            break\n",
    "\n",
    "            # filter out sub questions that failed\n",
    "            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n",
    "\n",
    "            nodes = [self._construct_node(pair) for pair in qa_pairs]\n",
    "\n",
    "            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n",
    "            response = self._response_synthesizer.synthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "                additional_source_nodes=source_nodes,\n",
    "            )\n",
    "\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "        yield response\n",
    "        #return response\n",
    "\n",
    "    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            sub_questions = await self._question_gen.agenerate(\n",
    "                self._metadatas, query_bundle\n",
    "            )\n",
    "\n",
    "            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n",
    "\n",
    "            if self._verbose:\n",
    "                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n",
    "\n",
    "            tasks = [\n",
    "                self._aquery_subq(sub_q, color=colors[str(ind)])\n",
    "                for ind, sub_q in enumerate(sub_questions)\n",
    "            ]\n",
    "\n",
    "            qa_pairs_all = await asyncio.gather(*tasks)\n",
    "            qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n",
    "\n",
    "            # filter out sub questions that failed\n",
    "            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n",
    "\n",
    "            nodes = [self._construct_node(pair) for pair in qa_pairs]\n",
    "\n",
    "            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n",
    "            response = await self._response_synthesizer.asynthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "                additional_source_nodes=source_nodes,\n",
    "            )\n",
    "\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _construct_node(self, qa_pair: SubQuestionAnswerPair) -> NodeWithScore:\n",
    "        node_text = (\n",
    "            f\"Sub question: {qa_pair.sub_q.sub_question}\\nResponse: {qa_pair.answer}\"\n",
    "        )\n",
    "        return NodeWithScore(node=TextNode(text=node_text))\n",
    "\n",
    "    async def _aquery_subq(\n",
    "        self, sub_q: SubQuestion, color: Optional[str] = None\n",
    "    ) -> Optional[SubQuestionAnswerPair]:\n",
    "        try:\n",
    "            with self.callback_manager.event(\n",
    "                CBEventType.SUB_QUESTION,\n",
    "                payload={EventPayload.SUB_QUESTION: SubQuestionAnswerPair(sub_q=sub_q)},\n",
    "            ) as event:\n",
    "                question = sub_q.sub_question\n",
    "                query_engine = self._query_engines[sub_q.tool_name]\n",
    "\n",
    "                if self._verbose:\n",
    "                    print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n",
    "\n",
    "                response = await query_engine.aquery(question)\n",
    "                response_text = str(response)\n",
    "\n",
    "                if self._verbose:\n",
    "                    print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n",
    "\n",
    "                qa_pair = SubQuestionAnswerPair(\n",
    "                    sub_q=sub_q, answer=response_text, sources=response.source_nodes\n",
    "                )\n",
    "\n",
    "                event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n",
    "\n",
    "            return qa_pair\n",
    "        except ValueError:\n",
    "            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n",
    "            return None\n",
    "\n",
    "    def _query_subq(\n",
    "        self, sub_q: SubQuestion, color: Optional[str] = None\n",
    "    ):\n",
    "        try:\n",
    "            question = sub_q.sub_question\n",
    "            query_engine = self._query_engines[sub_q.tool_name]\n",
    "            if self._verbose:\n",
    "                print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n",
    "            yield f\"[{sub_q.tool_name}] Q: {question}\\n\"\n",
    "            response = query_engine.query(question)\n",
    "            response_text = str(response)\n",
    "            if self._verbose:\n",
    "                print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n",
    "            yield f\"[{sub_q.tool_name}] A: {response_text}\\n\"#f\"[{sub_q.tool_name}] A: {response_text}\\n\"\n",
    "            qa_pair = SubQuestionAnswerPair(\n",
    "                sub_q=sub_q, answer=response_text, sources=response.source_nodes\n",
    "            )\n",
    "            #event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n",
    "            yield qa_pair\n",
    "            #print('test 1')\n",
    "            #return qa_pair\n",
    "        except ValueError:\n",
    "            #print('eeror')\n",
    "            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n",
    "            #yield None\n",
    "            #return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "d = {'d' : 1}\n",
    "print(list(d.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d.get('a'):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['','']) == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = ['tsst']\n",
    "print('\\n'.join(s[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join(s[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

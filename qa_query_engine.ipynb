{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from common.utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = Utils.get_tongyi_key()\n",
    "#llm = Tongyi(model_name= 'qwen-max-1201', temperature = 0)\n",
    "from common.CustomLLM import QwenLLM\n",
    "url = \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\"\n",
    "token = Utils.get_tongyi_key()\n",
    "\n",
    "qwen = QwenLLM(url= url, model='qwen-max-1201', token=token, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings' : True}\n",
    "embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name = model_name,\n",
    "    encode_kwargs = encode_kwargs,\n",
    "    query_instruction = \"Represent this sentence for searching relevant passages:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12381625175476074\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "embedding_model.embed_query(\"\"\"Dear Alumni,\n",
    "\n",
    "We would like to invite you to take part in the Universityâ€™s website user feedback questionnaire. Your thoughts and opinions are highly valued as crucial feedback for ongoing efforts to enhance the website. We appreciate your time and assure you that all responses will be kept strictly confidential.\n",
    "\n",
    "The questionnaire should take approximately 3 minutes to complete and can be accessed through the link below. As a token of our gratitude, the first 100 participants will receive a silicone travel tag as a reward. An email will be sent to winners and details of the redemption will be provided on 30 Jan 2024. Please kindly complete the questionnaire before 12 Jan 2024.\"\"\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine.sub_question_query_engine import SubQuestionQueryEngineCustom\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm = qwen, embed_model = embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs_report = SimpleDirectoryReader(input_files = ['./data/dbs-annual-report-2022.pdf']).load_data()#input_dir=\"./data/paul_graham/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index and query engine\n",
    "vector_dbs_idnex = VectorStoreIndex.from_documents(\n",
    "    dbs_report, use_async=True, service_context=service_context\n",
    ")\n",
    "\n",
    "vector_dbs_idnex.storage_context.persist(persist_dir = './data/subquery/dbs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.29\n"
     ]
    }
   ],
   "source": [
    "import llama_index\n",
    "print(llama_index.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_dbs_doc = StorageContext.from_defaults(persist_dir = './data/subquery/dbs')\n",
    "storage_dbs_index = load_index_from_storage(storage_dbs_doc, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=storage_dbs_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"DBS Holdings plc Annual Report and Accounts 2022\",\n",
    "            description=\"Provide information about DBS Group Holdings Ltd financials for year 2022\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngineCustom.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    verbose=True,\n",
    "    use_async=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query('what is the balance sheet of DBS in 2022, give the final result in markdown table format')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collin/miniconda3/envs/llm/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "Generated 3 Sub Questions.\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;237;90;200m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the total assets of DBS in 2022?\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the total assets of DBS in 2022?\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;237;90;200m[DBS Holdings plc Annual Report and Accounts 2022] A: The total assets of DBS Group Holdings Ltd and its subsidiaries as at 31 December 2022 is $743,368 million.\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] A: The total assets of DBS Group Holdings Ltd and its subsidiaries as at 31 December 2022 is $743,368 million.\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;90;149;237m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the total liabilities of DBS in 2022?\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the total liabilities of DBS in 2022?\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;90;149;237m[DBS Holdings plc Annual Report and Accounts 2022] A: I'm sorry, but the provided context information does not contain data on the total liabilities of DBS in 2022. It only provides information on the operational risk losses and reputational risk management at DBS, as well as details about the company's investments in subsidiaries and due from subsidiaries. To obtain the total liabilities of DBS in 2022, you would need to refer to the appropriate financial statements or reports that provide this information.\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] A: I'm sorry, but the provided context information does not contain data on the total liabilities of DBS in 2022. It only provides information on the operational risk losses and reputational risk management at DBS, as well as details about the company's investments in subsidiaries and due from subsidiaries. To obtain the total liabilities of DBS in 2022, you would need to refer to the appropriate financial statements or reports that provide this information.\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;11;159;203m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the equity of DBS in 2022?\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] Q: What is the equity of DBS in 2022?\n",
      "\n",
      "-----\n",
      "\u001b[1;3;38;2;11;159;203m[DBS Holdings plc Annual Report and Accounts 2022] A: The equity of DBS in 2022 is not explicitly stated in the provided context information. However, it can be inferred that the carrying amount of ordinary shares in subsidiaries is SGD 17,682 million and there are additional investments in other equity instruments amounting to SGD 344 million. The total investment in subsidiaries is SGD 21,008 million. Additionally, there are amounts due from subsidiaries such as subordinated term debts, other debt securities, and other receivables totaling SGD 8,532 million. Therefore, the total equity of DBS could potentially be calculated by adding these figures together, resulting in a rough estimate of SGD 29,540 million.\n",
      "\u001b[0m[DBS Holdings plc Annual Report and Accounts 2022] A: The equity of DBS in 2022 is not explicitly stated in the provided context information. However, it can be inferred that the carrying amount of ordinary shares in subsidiaries is SGD 17,682 million and there are additional investments in other equity instruments amounting to SGD 344 million. The total investment in subsidiaries is SGD 21,008 million. Additionally, there are amounts due from subsidiaries such as subordinated term debts, other debt securities, and other receivables totaling SGD 8,532 million. Therefore, the total equity of DBS could potentially be calculated by adding these figures together, resulting in a rough estimate of SGD 29,540 million.\n",
      "\n",
      "-----\n",
      "Based on the provided context information, we can construct a simplified balance sheet for DBS in 2022 as follows:\n",
      "\n",
      "| **Assets** | **Amount (SGD millions)** |\n",
      "| --- | --- |\n",
      "| Total Assets | 743,368 |\n",
      "| Investments in Subsidiaries and Due from Subsidiaries | 21,008 + 8,532 = 29,540 |\n",
      "| Other Equity Instruments | 344 |\n",
      "\n",
      "And,\n",
      "\n",
      "| **Liabilities and Equity** | **Amount (SGD millions)** |\n",
      "| --- | --- |\n",
      "| Operational Risk Losses | Unknown |\n",
      "| Reputational Risk Management | Unknown |\n",
      "| Ordinary Shares in Subsidiaries Carrying Amount | 17,682 |\n",
      "| Additional Investments in Other Equity Instruments | 344 |\n",
      "| Total Liabilities and Equity | N/A (*) |\n",
      "\n",
      "(*) To calculate the total liabilities and equity accurately, one needs more specific data about operational risk losses and reputational risk management at DBS.\n",
      "\n",
      "Note: This is a simplified representation based on limited context information. A complete and accurate balance sheet should include all relevant financial items and may require referring to official financial statements or reports published by DBS Group Holdings Ltd.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print('-' * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index==0.9.15.post2 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abd'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'abd'\n",
    "s.replace('a', '')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from typing import List, Optional, Sequence, cast\n",
    "\n",
    "from llama_index.async_utils import run_async_tasks\n",
    "from llama_index.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.callbacks.base import CallbackManager\n",
    "from llama_index.callbacks.schema import CBEventType, EventPayload\n",
    "from llama_index.core import BaseQueryEngine\n",
    "from llama_index.prompts.mixin import PromptMixinType\n",
    "from llama_index.question_gen.llm_generators import LLMQuestionGenerator\n",
    "from llama_index.question_gen.openai_generator import OpenAIQuestionGenerator\n",
    "from llama_index.question_gen.types import BaseQuestionGenerator, SubQuestion\n",
    "from llama_index.response.schema import RESPONSE_TYPE\n",
    "from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer\n",
    "from llama_index.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.service_context import ServiceContext\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "from llama_index.utils import get_color_mapping, print_text\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SubQuestionAnswerPair(BaseModel):\n",
    "    \"\"\"\n",
    "    Pair of the sub question and optionally its answer (if its been answered yet).\n",
    "    \"\"\"\n",
    "\n",
    "    sub_q: SubQuestion\n",
    "    answer: Optional[str] = None\n",
    "    sources: List[NodeWithScore] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class SubQuestionQueryEngineCustom(BaseQueryEngine):\n",
    "    \"\"\"Sub question query engine.\n",
    "\n",
    "    A query engine that breaks down a complex query (e.g. compare and contrast) into\n",
    "        many sub questions and their target query engine for execution.\n",
    "        After executing all sub questions, all responses are gathered and sent to\n",
    "        response synthesizer to produce the final response.\n",
    "\n",
    "    Args:\n",
    "        question_gen (BaseQuestionGenerator): A module for generating sub questions\n",
    "            given a complex question and tools.\n",
    "        response_synthesizer (BaseSynthesizer): A response synthesizer for\n",
    "            generating the final response\n",
    "        query_engine_tools (Sequence[QueryEngineTool]): Tools to answer the\n",
    "            sub questions.\n",
    "        verbose (bool): whether to print intermediate questions and answers.\n",
    "            Defaults to True\n",
    "        use_async (bool): whether to execute the sub questions with asyncio.\n",
    "            Defaults to True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        question_gen: BaseQuestionGenerator,\n",
    "        response_synthesizer: BaseSynthesizer,\n",
    "        query_engine_tools: Sequence[QueryEngineTool],\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        verbose: bool = True,\n",
    "        use_async: bool = False,\n",
    "    ) -> None:\n",
    "        self._question_gen = question_gen\n",
    "        self._response_synthesizer = response_synthesizer\n",
    "        self._metadatas = [x.metadata for x in query_engine_tools]\n",
    "        self._query_engines = {\n",
    "            tool.metadata.name: tool.query_engine for tool in query_engine_tools\n",
    "        }\n",
    "        self._verbose = verbose\n",
    "        self._use_async = use_async\n",
    "        super().__init__(callback_manager)\n",
    "\n",
    "    def _get_prompt_modules(self) -> PromptMixinType:\n",
    "        \"\"\"Get prompt sub-modules.\"\"\"\n",
    "        return {\n",
    "            \"question_gen\": self._question_gen,\n",
    "            \"response_synthesizer\": self._response_synthesizer,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        query_engine_tools: Sequence[QueryEngineTool],\n",
    "        question_gen: Optional[BaseQuestionGenerator] = None,\n",
    "        response_synthesizer: Optional[BaseSynthesizer] = None,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "        verbose: bool = True,\n",
    "        use_async: bool = True,\n",
    "    ) -> \"SubQuestionQueryEngine\":\n",
    "        callback_manager = None\n",
    "        if service_context is not None:\n",
    "            callback_manager = service_context.callback_manager\n",
    "        elif len(query_engine_tools) > 0:\n",
    "            callback_manager = query_engine_tools[0].query_engine.callback_manager\n",
    "\n",
    "        if question_gen is None:\n",
    "            if service_context is None:\n",
    "                # use default openai model that supports function calling API\n",
    "                question_gen = OpenAIQuestionGenerator.from_defaults()\n",
    "            else:\n",
    "                # try to use OpenAI function calling based question generator.\n",
    "                # if incompatible, use general LLM question generator\n",
    "                try:\n",
    "                    question_gen = OpenAIQuestionGenerator.from_defaults(\n",
    "                        llm=service_context.llm\n",
    "                    )\n",
    "                except ValueError:\n",
    "                    question_gen = LLMQuestionGenerator.from_defaults(\n",
    "                        service_context=service_context\n",
    "                    )\n",
    "\n",
    "        synth = response_synthesizer or get_response_synthesizer(\n",
    "            callback_manager=callback_manager,\n",
    "            service_context=service_context,\n",
    "            use_async=use_async,\n",
    "        )\n",
    "\n",
    "        return cls(\n",
    "            question_gen,\n",
    "            synth,\n",
    "            query_engine_tools,\n",
    "            callback_manager=callback_manager,\n",
    "            verbose=verbose,\n",
    "            use_async=use_async,\n",
    "        )\n",
    "\n",
    "    def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            sub_questions = self._question_gen.generate(self._metadatas, query_bundle)\n",
    "\n",
    "            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n",
    "\n",
    "            if self._verbose:\n",
    "                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n",
    "            yield f\"Generated {len(sub_questions)} Sub Questions.\\n\"\n",
    "\n",
    "            if self._use_async:\n",
    "                tasks = [\n",
    "                    self._aquery_subq(sub_q, color=colors[str(ind)])\n",
    "                    for ind, sub_q in enumerate(sub_questions)\n",
    "                ]\n",
    "\n",
    "                qa_pairs_all = run_async_tasks(tasks)\n",
    "                qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n",
    "            else:\n",
    "                qa_pairs_all = []\n",
    "                #    self._query_subq(sub_q, color=colors[str(ind)])\n",
    "                #    for ind, sub_q in enumerate(sub_questions)\n",
    "                #]\n",
    "                #print(\"Question\", sub_questions)\n",
    "                for ind, sub_q in enumerate(sub_questions):\n",
    "                    for item in self._query_subq(sub_q, color=colors[str(ind)]):\n",
    "                        #print(ind, item)\n",
    "                        if isinstance(item, str):\n",
    "                            yield item\n",
    "                        else:\n",
    "                            #print('itemss')\n",
    "                            qa_pairs_all.append(item)\n",
    "                            break\n",
    "\n",
    "            # filter out sub questions that failed\n",
    "            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n",
    "\n",
    "            nodes = [self._construct_node(pair) for pair in qa_pairs]\n",
    "\n",
    "            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n",
    "            response = self._response_synthesizer.synthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "                additional_source_nodes=source_nodes,\n",
    "            )\n",
    "\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "        yield response\n",
    "        #return response\n",
    "\n",
    "    async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n",
    "        with self.callback_manager.event(\n",
    "            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n",
    "        ) as query_event:\n",
    "            sub_questions = await self._question_gen.agenerate(\n",
    "                self._metadatas, query_bundle\n",
    "            )\n",
    "\n",
    "            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])\n",
    "\n",
    "            if self._verbose:\n",
    "                print_text(f\"Generated {len(sub_questions)} sub questions.\\n\")\n",
    "\n",
    "            tasks = [\n",
    "                self._aquery_subq(sub_q, color=colors[str(ind)])\n",
    "                for ind, sub_q in enumerate(sub_questions)\n",
    "            ]\n",
    "\n",
    "            qa_pairs_all = await asyncio.gather(*tasks)\n",
    "            qa_pairs_all = cast(List[Optional[SubQuestionAnswerPair]], qa_pairs_all)\n",
    "\n",
    "            # filter out sub questions that failed\n",
    "            qa_pairs: List[SubQuestionAnswerPair] = list(filter(None, qa_pairs_all))\n",
    "\n",
    "            nodes = [self._construct_node(pair) for pair in qa_pairs]\n",
    "\n",
    "            source_nodes = [node for qa_pair in qa_pairs for node in qa_pair.sources]\n",
    "            response = await self._response_synthesizer.asynthesize(\n",
    "                query=query_bundle,\n",
    "                nodes=nodes,\n",
    "                additional_source_nodes=source_nodes,\n",
    "            )\n",
    "\n",
    "            query_event.on_end(payload={EventPayload.RESPONSE: response})\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _construct_node(self, qa_pair: SubQuestionAnswerPair) -> NodeWithScore:\n",
    "        node_text = (\n",
    "            f\"Sub question: {qa_pair.sub_q.sub_question}\\nResponse: {qa_pair.answer}\"\n",
    "        )\n",
    "        return NodeWithScore(node=TextNode(text=node_text))\n",
    "\n",
    "    async def _aquery_subq(\n",
    "        self, sub_q: SubQuestion, color: Optional[str] = None\n",
    "    ) -> Optional[SubQuestionAnswerPair]:\n",
    "        try:\n",
    "            with self.callback_manager.event(\n",
    "                CBEventType.SUB_QUESTION,\n",
    "                payload={EventPayload.SUB_QUESTION: SubQuestionAnswerPair(sub_q=sub_q)},\n",
    "            ) as event:\n",
    "                question = sub_q.sub_question\n",
    "                query_engine = self._query_engines[sub_q.tool_name]\n",
    "\n",
    "                if self._verbose:\n",
    "                    print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n",
    "\n",
    "                response = await query_engine.aquery(question)\n",
    "                response_text = str(response)\n",
    "\n",
    "                if self._verbose:\n",
    "                    print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n",
    "\n",
    "                qa_pair = SubQuestionAnswerPair(\n",
    "                    sub_q=sub_q, answer=response_text, sources=response.source_nodes\n",
    "                )\n",
    "\n",
    "                event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n",
    "\n",
    "            return qa_pair\n",
    "        except ValueError:\n",
    "            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n",
    "            return None\n",
    "\n",
    "    def _query_subq(\n",
    "        self, sub_q: SubQuestion, color: Optional[str] = None\n",
    "    ):\n",
    "        try:\n",
    "            question = sub_q.sub_question\n",
    "            query_engine = self._query_engines[sub_q.tool_name]\n",
    "            if self._verbose:\n",
    "                print_text(f\"[{sub_q.tool_name}] Q: {question}\\n\", color=color)\n",
    "            yield f\"[{sub_q.tool_name}] Q: {question}\\n\"\n",
    "            response = query_engine.query(question)\n",
    "            response_text = str(response)\n",
    "            if self._verbose:\n",
    "                print_text(f\"[{sub_q.tool_name}] A: {response_text}\\n\", color=color)\n",
    "            yield f\"[{sub_q.tool_name}] A: {response_text}\\n\"#f\"[{sub_q.tool_name}] A: {response_text}\\n\"\n",
    "            qa_pair = SubQuestionAnswerPair(\n",
    "                sub_q=sub_q, answer=response_text, sources=response.source_nodes\n",
    "            )\n",
    "            #event.on_end(payload={EventPayload.SUB_QUESTION: qa_pair})\n",
    "            yield qa_pair\n",
    "            #print('test 1')\n",
    "            #return qa_pair\n",
    "        except ValueError:\n",
    "            #print('eeror')\n",
    "            logger.warning(f\"[{sub_q.tool_name}] Failed to run {question}\")\n",
    "            #yield None\n",
    "            #return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "d = {'d' : 1}\n",
    "print(list(d.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d.get('a'):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['','']) == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = ['tsst']\n",
    "print('\\n'.join(s[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join(s[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
